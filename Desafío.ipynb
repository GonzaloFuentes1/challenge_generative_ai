{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vllm unidecode langdetect pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from unidecode import unidecode\n",
    "from vllm import LLM, SamplingParams\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer\n",
    "from langdetect import detect, detect_langs\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# ------------ CONFIG ------------\n",
    "MODELO = \"google/gemma-3-4b-it\"   # repo HF o ruta local meta-llama/Llama-3.1-8B-Instruct TinyLlama/TinyLlama-1.1B-Chat-v1.0 Qwen/Qwen3-4B-Instruct-2507\n",
    "CANTIDAD = 20000\n",
    "MAX_PREGUNTAS_FILTRAR = 20000  # Limitar el número de preguntas a procesar para filtrar\n",
    "BENCHMARK_FILE = \"gonza_qa.json\"\n",
    "OUTPUT_FILE = f\"gonza_evaluacion_real_{MODELO.split('/')[-1]}.json\"\n",
    "MODELS_DIR = Path(\"./models\")                   # dónde guardar los modelos\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"\"\n",
    "# ---------------------------------\n",
    "\n",
    "print(f\"Configuración:\")\n",
    "print(f\"- Modelo: {MODELO}\")\n",
    "print(f\"- Cantidad de preguntas para evaluación: {CANTIDAD}\")\n",
    "print(f\"- Máximo de preguntas a procesar para filtrar: {MAX_PREGUNTAS_FILTRAR}\")\n",
    "print(f\"- Archivo benchmark: {BENCHMARK_FILE}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Ejemplos few-shot específicos de Latinoamérica\n",
    "FEW_SHOT_EXAMPLES = [\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Argentina?\",\n",
    "        \"respuesta\": \"Buenos Aires\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué año se independizó México?\",\n",
    "        \"respuesta\": \"1810\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la moneda oficial de Chile?\",\n",
    "        \"respuesta\": \"Peso chileno\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Quién fue el libertador de Venezuela?\",\n",
    "        \"respuesta\": \"Simón Bolívar\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es el río más largo de Sudamérica?\",\n",
    "        \"respuesta\": \"Río Amazonas\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué país se encuentra Machu Picchu?\",\n",
    "        \"respuesta\": \"Perú\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Colombia?\",\n",
    "        \"respuesta\": \"Bogotá\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Qué escritor colombiano ganó el Premio Nobel de Literatura?\",\n",
    "        \"respuesta\": \"Gabriel García Márquez\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es el país más grande de América del Sur?\",\n",
    "        \"respuesta\": \"Brasil\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué océano se encuentran las Islas Galápagos?\",\n",
    "        \"respuesta\": \"Océano Pacífico\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Ecuador?\",\n",
    "        \"respuesta\": \"Quito\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Qué cordillera atraviesa América del Sur?\",\n",
    "        \"respuesta\": \"Cordillera de los Andes\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la moneda oficial de Brasil?\",\n",
    "        \"respuesta\": \"Real brasileño\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué país nació el tango?\",\n",
    "        \"respuesta\": \"Argentina\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Uruguay?\",\n",
    "        \"respuesta\": \"Montevideo\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Qué país tiene forma de bota en América del Sur?\",\n",
    "        \"respuesta\": \"Chile\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es el desierto más árido del mundo ubicado en Chile?\",\n",
    "        \"respuesta\": \"Desierto de Atacama\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué país se encuentra el Salar de Uyuni?\",\n",
    "        \"respuesta\": \"Bolivia\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Paraguay?\",\n",
    "        \"respuesta\": \"Asunción\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Qué idioma se habla en Brasil?\",\n",
    "        \"respuesta\": \"Portugués\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def is_local_path(p: str) -> bool:\n",
    "    return Path(p).expanduser().exists()\n",
    "\n",
    "def sanitize(name: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in \"._-+=\" else \"_\" for ch in name)\n",
    "\n",
    "def ensure_local_model(model_id_or_path: str, local_dir: Path) -> str:\n",
    "    if is_local_path(model_id_or_path):\n",
    "        return str(Path(model_id_or_path).expanduser().resolve())\n",
    "    \n",
    "    local_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dst = local_dir / sanitize(model_id_or_path)\n",
    "    \n",
    "    # Bypass específico para google/gemma-3-4b-it\n",
    "    if \"google/gemma\" in model_id_or_path.lower():\n",
    "        print(f\"Aplicando bypass para modelo Gemma: {model_id_or_path}\")\n",
    "        try:\n",
    "            # Intentar descarga con patrones más amplios para Gemma\n",
    "            snapshot_download(\n",
    "                repo_id=model_id_or_path,\n",
    "                local_dir=str(dst),\n",
    "                local_dir_use_symlinks=False,\n",
    "                # Patrones más amplios para Gemma\n",
    "                allow_patterns=[\n",
    "                    \"*.json\", \"*.txt\", \"*.model\", \"*.safetensors\", \"*.bin\",\n",
    "                    \"tokenizer*\", \"config*\", \"generation_config*\",\n",
    "                    \"model*\", \"pytorch_model*\", \"special_tokens_map*\"\n",
    "                ],\n",
    "            )\n",
    "        except Exception as e1:\n",
    "            print(f\"Primer intento falló: {e1}\")\n",
    "            try:\n",
    "                # Segundo intento: descarga completa sin filtros\n",
    "                print(\"Intentando descarga completa sin filtros...\")\n",
    "                snapshot_download(\n",
    "                    repo_id=model_id_or_path,\n",
    "                    local_dir=str(dst),\n",
    "                    local_dir_use_symlinks=False,\n",
    "                )\n",
    "            except Exception as e2:\n",
    "                print(f\"Segundo intento falló: {e2}\")\n",
    "                # Tercer intento: usar directamente el repo_id para vLLM\n",
    "                print(\"Usando repo_id directamente para vLLM...\")\n",
    "                return model_id_or_path\n",
    "    else:\n",
    "        # Para otros modelos, usar el método original\n",
    "        snapshot_download(\n",
    "            repo_id=model_id_or_path,\n",
    "            local_dir=str(dst),\n",
    "            local_dir_use_symlinks=False,\n",
    "            allow_patterns=[\n",
    "                \"config.json\", \"generation_config.json\",\n",
    "                \"tokenizer.json\", \"tokenizer.model\", \"tokenizer_config.json\",\n",
    "                \"special_tokens_map.json\", \"merges.txt\", \"vocab.json\",\n",
    "                \"*.safetensors\", \"model.safetensors.index.json\", \"pytorch_model.bin\",\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "    return str(dst)\n",
    "\n",
    "def is_spanish_text(text: str) -> bool:\n",
    "    \"\"\"Detecta si el texto está en español\"\"\"\n",
    "    if not text or len(text.strip()) < 3:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Usar detect_langs para obtener probabilidades\n",
    "        languages = detect_langs(text)\n",
    "        \n",
    "        # Buscar español en los resultados\n",
    "        for lang in languages:\n",
    "            if lang.lang == 'es' and lang.prob > 0.55:\n",
    "                return True\n",
    "        \n",
    "        # Si no hay alta confianza, usar detect simple\n",
    "        detected_lang = detect(text)\n",
    "        return detected_lang == 'es'\n",
    "        \n",
    "    except:\n",
    "        # Si langdetect falla completamente, considerar como no español\n",
    "        return False\n",
    "\n",
    "def filter_spanish_questions_sequential(benchmark_data, max_items=None):\n",
    "    \"\"\"Filtra preguntas que estén completamente en español usando procesamiento secuencial\"\"\"\n",
    "    filtered_data = []\n",
    "    filtered_count = 0\n",
    "    \n",
    "    # Limitar el número de items a procesar si se especifica\n",
    "    if max_items and max_items < len(benchmark_data):\n",
    "        print(f\"Limitando procesamiento a {max_items} de {len(benchmark_data)} preguntas totales\")\n",
    "        data_to_process = benchmark_data[:max_items]\n",
    "    else:\n",
    "        data_to_process = benchmark_data\n",
    "    \n",
    "    print(f\"Filtrando {len(data_to_process)} preguntas...\")\n",
    "    \n",
    "    for item in tqdm(data_to_process, desc=\"Filtrando preguntas\"):\n",
    "        pregunta = item.get('pregunta', '')\n",
    "        respuesta = item.get('respuesta_correcta', '')\n",
    "        \n",
    "        # Verificar que tanto pregunta como respuesta estén en español\n",
    "        if is_spanish_text(pregunta) and is_spanish_text(respuesta):\n",
    "            filtered_data.append(item)\n",
    "        else:\n",
    "            filtered_count += 1\n",
    "    \n",
    "    print(f\"Filtradas {filtered_count} preguntas (no español)\")\n",
    "    print(f\"Preguntas válidas en español: {len(filtered_data)}\")\n",
    "    return filtered_data\n",
    "\n",
    "def normalizar_texto(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return \"\"\n",
    "    texto = texto.lower()\n",
    "    texto = unidecode(texto)\n",
    "    texto = texto.translate(str.maketrans('', '', string.punctuation))\n",
    "    return texto.strip()\n",
    "\n",
    "# --- Carga benchmark ---\n",
    "if not os.path.exists(BENCHMARK_FILE):\n",
    "    print(f\"Error: No se encuentra '{BENCHMARK_FILE}'. Ejecuta el script 2 primero.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "with open(BENCHMARK_FILE, 'r', encoding='utf-8') as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "if len(benchmark_data) == 0:\n",
    "    print(\"Error: El benchmark está vacío.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Filtrar preguntas en español usando procesamiento secuencial\n",
    "print(f\"Datos originales: {len(benchmark_data)} preguntas\")\n",
    "# benchmark_data_spanish = filter_spanish_questions_sequential(benchmark_data, MAX_PREGUNTAS_FILTRAR)\n",
    "benchmark_data_spanish = benchmark_data\n",
    "print(f\"Después del filtro de español: {len(benchmark_data_spanish)} preguntas\")\n",
    "\n",
    "if len(benchmark_data_spanish) == 0:\n",
    "    print(\"Error: No hay preguntas válidas en español después del filtro.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Verificar si tenemos suficientes preguntas para la muestra\n",
    "if len(benchmark_data_spanish) < CANTIDAD:\n",
    "    print(f\"Advertencia: Solo hay {len(benchmark_data_spanish)} preguntas válidas, usando todas para la evaluación.\")\n",
    "    CANTIDAD = len(benchmark_data_spanish)\n",
    "\n",
    "benchmark_sample = random.sample(benchmark_data_spanish, min(len(benchmark_data_spanish), CANTIDAD))\n",
    "print(f\"Muestra seleccionada: {len(benchmark_sample)} preguntas\")\n",
    "\n",
    "# --- Modelo local (descarga si hace falta) ---\n",
    "modelo_local = ensure_local_model(MODELO, MODELS_DIR)\n",
    "print(f\"Modelo preparado en: {modelo_local}\")\n",
    "\n",
    "# --- Tokenizer (para chat template si existe) ---\n",
    "# Bypass para tokenizer de Gemma\n",
    "if \"google/gemma\" in MODELO.lower() and modelo_local == MODELO:\n",
    "    # Si no se pudo descargar localmente, usar directamente el repo\n",
    "    print(\"Cargando tokenizer directamente desde Hugging Face...\")\n",
    "    tok = AutoTokenizer.from_pretrained(\n",
    "        MODELO,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    tok = AutoTokenizer.from_pretrained(\n",
    "        modelo_local,\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True\n",
    "    )\n",
    "\n",
    "def create_few_shot_prompt(pregunta: str) -> str:\n",
    "    \"\"\"Crea un prompt con ejemplos few-shot de Latinoamérica\"\"\"\n",
    "    system_message = \"\"\"Eres un asistente experto en conocimientos de Latinoamérica. Responde de manera precisa y concisa en español.\n",
    "\n",
    "Ejemplos de preguntas y respuestas sobre Latinoamérica:\"\"\"\n",
    "    \n",
    "    # Agregar ejemplos few-shot\n",
    "    examples_text = \"\"\n",
    "    for example in FEW_SHOT_EXAMPLES:\n",
    "        examples_text += f\"\\nPregunta: {example['pregunta']}\\nRespuesta: {example['respuesta']}\\n\"\n",
    "    \n",
    "    user_message = f\"\\nAhora responde esta pregunta:\\nPregunta: {pregunta}\\nRespuesta:\"\n",
    "    \n",
    "    return system_message + examples_text + user_message\n",
    "\n",
    "def format_prompt(pregunta: str) -> str:\n",
    "    few_shot_content = create_few_shot_prompt(pregunta)\n",
    "    \n",
    "    if hasattr(tok, \"apply_chat_template\") and (getattr(tok, \"chat_template\", None) is not None):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Eres un asistente experto en conocimientos de Latinoamérica.\"},\n",
    "            {\"role\": \"user\", \"content\": few_shot_content}\n",
    "        ]\n",
    "        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return f\"<|user|>\\n{few_shot_content}<|assistant|>\"\n",
    "\n",
    "prompts = [format_prompt(item['pregunta']) for item in benchmark_sample]\n",
    "respuestas_correctas_sample = [item['respuesta_correcta'] for item in benchmark_sample]\n",
    "\n",
    "print(f\"\\nCargando modelo '{MODELO}' en la GPU...\")\n",
    "sampling_params = SamplingParams(temperature=0.1, top_p=0.9, max_tokens=128)\n",
    "\n",
    "# Configuración específica para Gemma\n",
    "if \"google/gemma\" in MODELO.lower():\n",
    "    print(\"Configurando vLLM para modelo Gemma...\")\n",
    "    try:\n",
    "        llm = LLM(\n",
    "            model=modelo_local, \n",
    "            gpu_memory_utilization=0.7, \n",
    "            dtype=\"float16\", \n",
    "            trust_remote_code=True,\n",
    "            # Configuraciones adicionales para Gemma\n",
    "            max_model_len=4096,\n",
    "            enforce_eager=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error con configuración específica de Gemma: {e}\")\n",
    "        print(\"Intentando con configuración estándar...\")\n",
    "        llm = LLM(model=modelo_local, gpu_memory_utilization=0.7, dtype=\"float16\", trust_remote_code=True)\n",
    "else:\n",
    "    llm = LLM(model=modelo_local, gpu_memory_utilization=0.7, dtype=\"float16\", trust_remote_code=True)\n",
    "\n",
    "print(\"¡Modelo cargado! Iniciando generación de respuestas...\")\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "print(\"¡Generación completa! Evaluando respuestas...\")\n",
    "\n",
    "total_preguntas = 0\n",
    "correctas = 0\n",
    "resultados_evaluacion = []\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    pregunta_original = benchmark_sample[i]['pregunta']\n",
    "    respuesta_llm = output.outputs[0].text.strip()\n",
    "    respuesta_correcta_gt = respuestas_correctas_sample[i]\n",
    "\n",
    "    # Verificar que la respuesta del LLM esté en español\n",
    "    respuesta_en_espanol = respuesta_llm\n",
    "    \n",
    "    norm_correcta = normalizar_texto(respuesta_correcta_gt)\n",
    "    norm_llm = normalizar_texto(respuesta_llm)\n",
    "    es_correcta = (norm_correcta in norm_llm) and (norm_correcta != \"\") and respuesta_en_espanol\n",
    "\n",
    "    if es_correcta:\n",
    "        correctas += 1\n",
    "    total_preguntas += 1\n",
    "\n",
    "    evaluacion = \"CORRECTO\" if es_correcta else \"INCORRECTO\"\n",
    "    if not respuesta_en_espanol:\n",
    "        evaluacion += \" (No español)\"\n",
    "\n",
    "    resultados_evaluacion.append({\n",
    "        \"pregunta\": pregunta_original,\n",
    "        \"respuesta_correcta\": respuesta_correcta_gt,\n",
    "        \"respuesta_llm\": respuesta_llm,\n",
    "        \"evaluacion\": evaluacion,\n",
    "        \"respuesta_en_espanol\": respuesta_en_espanol\n",
    "    })\n",
    "\n",
    "print(\"\\n--- Tabla de Resultados de Evaluación (Muestra de 10) ---\")\n",
    "for res in resultados_evaluacion[:10]:\n",
    "    print(f\"  P: {res['pregunta']}\")\n",
    "    print(f\"  R. Correcta: {res['respuesta_correcta']}\")\n",
    "    print(f\"  R. LLM: {res['respuesta_llm']} -> {res['evaluacion']}\")\n",
    "    print(\"  ---\")\n",
    "\n",
    "if total_preguntas > 0:\n",
    "    puntaje = (correctas / total_preguntas) * 100\n",
    "    respuestas_espanol = sum(1 for r in resultados_evaluacion if r['respuesta_en_espanol'])\n",
    "    print(f\"\\n Resultado Final ({MODELO}): {correctas} de {total_preguntas} correctas ({puntaje:.1f}%)\")\n",
    "    print(f\" Respuestas en español: {respuestas_espanol} de {total_preguntas} ({(respuestas_espanol/total_preguntas)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"No se evaluaron preguntas.\")\n",
    "\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_evaluacion, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Resultados detallados guardados en '{OUTPUT_FILE}'\")\n",
    "print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from unidecode import unidecode\n",
    "from vllm import LLM, SamplingParams\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer\n",
    "from langdetect import detect, detect_langs\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# ------------ CONFIG ------------\n",
    "MODELO = \"google/gemma-3-4b-it\"   # repo HF o ruta local meta-llama/Llama-3.1-8B-Instruct TinyLlama/TinyLlama-1.1B-Chat-v1.0 Qwen/Qwen3-4B-Instruct-2507\n",
    "CANTIDAD = 20000\n",
    "MAX_PREGUNTAS_FILTRAR = 20000  # Limitar el número de preguntas a procesar para filtrar\n",
    "BENCHMARK_FILE = \"usa_qa.json\"\n",
    "OUTPUT_FILE = f\"usa_evaluacion_real_{MODELO.split('/')[-1]}.json\"\n",
    "MODELS_DIR = Path(\"./models\")                   # dónde guardar los modelos\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"\"\n",
    "# ---------------------------------\n",
    "\n",
    "print(f\"Configuración:\")\n",
    "print(f\"- Modelo: {MODELO}\")\n",
    "print(f\"- Cantidad de preguntas para evaluación: {CANTIDAD}\")\n",
    "print(f\"- Máximo de preguntas a procesar para filtrar: {MAX_PREGUNTAS_FILTRAR}\")\n",
    "print(f\"- Archivo benchmark: {BENCHMARK_FILE}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Ejemplos few-shot específicos de Latinoamérica\n",
    "FEW_SHOT_EXAMPLES = [\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Argentina?\",\n",
    "        \"respuesta\": \"Buenos Aires\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué año se independizó México?\",\n",
    "        \"respuesta\": \"1810\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la moneda oficial de Chile?\",\n",
    "        \"respuesta\": \"Peso chileno\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Quién fue el libertador de Venezuela?\",\n",
    "        \"respuesta\": \"Simón Bolívar\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es el río más largo de Sudamérica?\",\n",
    "        \"respuesta\": \"Río Amazonas\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué país se encuentra Machu Picchu?\",\n",
    "        \"respuesta\": \"Perú\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Colombia?\",\n",
    "        \"respuesta\": \"Bogotá\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Qué escritor colombiano ganó el Premio Nobel de Literatura?\",\n",
    "        \"respuesta\": \"Gabriel García Márquez\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es el país más grande de América del Sur?\",\n",
    "        \"respuesta\": \"Brasil\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué océano se encuentran las Islas Galápagos?\",\n",
    "        \"respuesta\": \"Océano Pacífico\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Ecuador?\",\n",
    "        \"respuesta\": \"Quito\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Qué cordillera atraviesa América del Sur?\",\n",
    "        \"respuesta\": \"Cordillera de los Andes\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la moneda oficial de Brasil?\",\n",
    "        \"respuesta\": \"Real brasileño\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué país nació el tango?\",\n",
    "        \"respuesta\": \"Argentina\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Uruguay?\",\n",
    "        \"respuesta\": \"Montevideo\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Qué país tiene forma de bota en América del Sur?\",\n",
    "        \"respuesta\": \"Chile\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es el desierto más árido del mundo ubicado en Chile?\",\n",
    "        \"respuesta\": \"Desierto de Atacama\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué país se encuentra el Salar de Uyuni?\",\n",
    "        \"respuesta\": \"Bolivia\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Paraguay?\",\n",
    "        \"respuesta\": \"Asunción\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Qué idioma se habla en Brasil?\",\n",
    "        \"respuesta\": \"Portugués\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def is_local_path(p: str) -> bool:\n",
    "    return Path(p).expanduser().exists()\n",
    "\n",
    "def sanitize(name: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in \"._-+=\" else \"_\" for ch in name)\n",
    "\n",
    "def ensure_local_model(model_id_or_path: str, local_dir: Path) -> str:\n",
    "    if is_local_path(model_id_or_path):\n",
    "        return str(Path(model_id_or_path).expanduser().resolve())\n",
    "    \n",
    "    local_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dst = local_dir / sanitize(model_id_or_path)\n",
    "    \n",
    "    # Bypass específico para google/gemma-3-4b-it\n",
    "    if \"google/gemma\" in model_id_or_path.lower():\n",
    "        print(f\"Aplicando bypass para modelo Gemma: {model_id_or_path}\")\n",
    "        try:\n",
    "            # Intentar descarga con patrones más amplios para Gemma\n",
    "            snapshot_download(\n",
    "                repo_id=model_id_or_path,\n",
    "                local_dir=str(dst),\n",
    "                local_dir_use_symlinks=False,\n",
    "                # Patrones más amplios para Gemma\n",
    "                allow_patterns=[\n",
    "                    \"*.json\", \"*.txt\", \"*.model\", \"*.safetensors\", \"*.bin\",\n",
    "                    \"tokenizer*\", \"config*\", \"generation_config*\",\n",
    "                    \"model*\", \"pytorch_model*\", \"special_tokens_map*\"\n",
    "                ],\n",
    "            )\n",
    "        except Exception as e1:\n",
    "            print(f\"Primer intento falló: {e1}\")\n",
    "            try:\n",
    "                # Segundo intento: descarga completa sin filtros\n",
    "                print(\"Intentando descarga completa sin filtros...\")\n",
    "                snapshot_download(\n",
    "                    repo_id=model_id_or_path,\n",
    "                    local_dir=str(dst),\n",
    "                    local_dir_use_symlinks=False,\n",
    "                )\n",
    "            except Exception as e2:\n",
    "                print(f\"Segundo intento falló: {e2}\")\n",
    "                # Tercer intento: usar directamente el repo_id para vLLM\n",
    "                print(\"Usando repo_id directamente para vLLM...\")\n",
    "                return model_id_or_path\n",
    "    else:\n",
    "        # Para otros modelos, usar el método original\n",
    "        snapshot_download(\n",
    "            repo_id=model_id_or_path,\n",
    "            local_dir=str(dst),\n",
    "            local_dir_use_symlinks=False,\n",
    "            allow_patterns=[\n",
    "                \"config.json\", \"generation_config.json\",\n",
    "                \"tokenizer.json\", \"tokenizer.model\", \"tokenizer_config.json\",\n",
    "                \"special_tokens_map.json\", \"merges.txt\", \"vocab.json\",\n",
    "                \"*.safetensors\", \"model.safetensors.index.json\", \"pytorch_model.bin\",\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "    return str(dst)\n",
    "\n",
    "def is_spanish_text(text: str) -> bool:\n",
    "    \"\"\"Detecta si el texto está en español\"\"\"\n",
    "    if not text or len(text.strip()) < 3:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Usar detect_langs para obtener probabilidades\n",
    "        languages = detect_langs(text)\n",
    "        \n",
    "        # Buscar español en los resultados\n",
    "        for lang in languages:\n",
    "            if lang.lang == 'es' and lang.prob > 0.55:\n",
    "                return True\n",
    "        \n",
    "        # Si no hay alta confianza, usar detect simple\n",
    "        detected_lang = detect(text)\n",
    "        return detected_lang == 'es'\n",
    "        \n",
    "    except:\n",
    "        # Si langdetect falla completamente, considerar como no español\n",
    "        return False\n",
    "\n",
    "def filter_spanish_questions_sequential(benchmark_data, max_items=None):\n",
    "    \"\"\"Filtra preguntas que estén completamente en español usando procesamiento secuencial\"\"\"\n",
    "    filtered_data = []\n",
    "    filtered_count = 0\n",
    "    \n",
    "    # Limitar el número de items a procesar si se especifica\n",
    "    if max_items and max_items < len(benchmark_data):\n",
    "        print(f\"Limitando procesamiento a {max_items} de {len(benchmark_data)} preguntas totales\")\n",
    "        data_to_process = benchmark_data[:max_items]\n",
    "    else:\n",
    "        data_to_process = benchmark_data\n",
    "    \n",
    "    print(f\"Filtrando {len(data_to_process)} preguntas...\")\n",
    "    \n",
    "    for item in tqdm(data_to_process, desc=\"Filtrando preguntas\"):\n",
    "        pregunta = item.get('pregunta', '')\n",
    "        respuesta = item.get('respuesta_correcta', '')\n",
    "        \n",
    "        # Verificar que tanto pregunta como respuesta estén en español\n",
    "        if is_spanish_text(pregunta) and is_spanish_text(respuesta):\n",
    "            filtered_data.append(item)\n",
    "        else:\n",
    "            filtered_count += 1\n",
    "    \n",
    "    print(f\"Filtradas {filtered_count} preguntas (no español)\")\n",
    "    print(f\"Preguntas válidas en español: {len(filtered_data)}\")\n",
    "    return filtered_data\n",
    "\n",
    "def normalizar_texto(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return \"\"\n",
    "    texto = texto.lower()\n",
    "    texto = unidecode(texto)\n",
    "    texto = texto.translate(str.maketrans('', '', string.punctuation))\n",
    "    return texto.strip()\n",
    "\n",
    "# --- Carga benchmark ---\n",
    "if not os.path.exists(BENCHMARK_FILE):\n",
    "    print(f\"Error: No se encuentra '{BENCHMARK_FILE}'. Ejecuta el script 2 primero.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "with open(BENCHMARK_FILE, 'r', encoding='utf-8') as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "if len(benchmark_data) == 0:\n",
    "    print(\"Error: El benchmark está vacío.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Filtrar preguntas en español usando procesamiento secuencial\n",
    "print(f\"Datos originales: {len(benchmark_data)} preguntas\")\n",
    "# benchmark_data_spanish = filter_spanish_questions_sequential(benchmark_data, MAX_PREGUNTAS_FILTRAR)\n",
    "benchmark_data_spanish = benchmark_data\n",
    "print(f\"Después del filtro de español: {len(benchmark_data_spanish)} preguntas\")\n",
    "\n",
    "if len(benchmark_data_spanish) == 0:\n",
    "    print(\"Error: No hay preguntas válidas en español después del filtro.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Verificar si tenemos suficientes preguntas para la muestra\n",
    "if len(benchmark_data_spanish) < CANTIDAD:\n",
    "    print(f\"Advertencia: Solo hay {len(benchmark_data_spanish)} preguntas válidas, usando todas para la evaluación.\")\n",
    "    CANTIDAD = len(benchmark_data_spanish)\n",
    "\n",
    "benchmark_sample = random.sample(benchmark_data_spanish, min(len(benchmark_data_spanish), CANTIDAD))\n",
    "print(f\"Muestra seleccionada: {len(benchmark_sample)} preguntas\")\n",
    "\n",
    "# --- Modelo local (descarga si hace falta) ---\n",
    "modelo_local = ensure_local_model(MODELO, MODELS_DIR)\n",
    "print(f\"Modelo preparado en: {modelo_local}\")\n",
    "\n",
    "# --- Tokenizer (para chat template si existe) ---\n",
    "# Bypass para tokenizer de Gemma\n",
    "if \"google/gemma\" in MODELO.lower() and modelo_local == MODELO:\n",
    "    # Si no se pudo descargar localmente, usar directamente el repo\n",
    "    print(\"Cargando tokenizer directamente desde Hugging Face...\")\n",
    "    tok = AutoTokenizer.from_pretrained(\n",
    "        MODELO,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    tok = AutoTokenizer.from_pretrained(\n",
    "        modelo_local,\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True\n",
    "    )\n",
    "\n",
    "def create_few_shot_prompt(pregunta: str) -> str:\n",
    "    \"\"\"Crea un prompt con ejemplos few-shot de Latinoamérica\"\"\"\n",
    "    system_message = \"\"\"Eres un asistente experto en conocimientos de Latinoamérica. Responde de manera precisa y concisa en español.\n",
    "\n",
    "Ejemplos de preguntas y respuestas sobre Latinoamérica:\"\"\"\n",
    "    \n",
    "    # Agregar ejemplos few-shot\n",
    "    examples_text = \"\"\n",
    "    for example in FEW_SHOT_EXAMPLES:\n",
    "        examples_text += f\"\\nPregunta: {example['pregunta']}\\nRespuesta: {example['respuesta']}\\n\"\n",
    "    \n",
    "    user_message = f\"\\nAhora responde esta pregunta:\\nPregunta: {pregunta}\\nRespuesta:\"\n",
    "    \n",
    "    return system_message + examples_text + user_message\n",
    "\n",
    "def format_prompt(pregunta: str) -> str:\n",
    "    few_shot_content = create_few_shot_prompt(pregunta)\n",
    "    \n",
    "    if hasattr(tok, \"apply_chat_template\") and (getattr(tok, \"chat_template\", None) is not None):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Eres un asistente experto en conocimientos de Latinoamérica.\"},\n",
    "            {\"role\": \"user\", \"content\": few_shot_content}\n",
    "        ]\n",
    "        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return f\"<|user|>\\n{few_shot_content}<|assistant|>\"\n",
    "\n",
    "prompts = [format_prompt(item['pregunta']) for item in benchmark_sample]\n",
    "respuestas_correctas_sample = [item['respuesta_correcta'] for item in benchmark_sample]\n",
    "\n",
    "print(f\"\\nCargando modelo '{MODELO}' en la GPU...\")\n",
    "sampling_params = SamplingParams(temperature=0.1, top_p=0.9, max_tokens=128)\n",
    "\n",
    "# Configuración específica para Gemma\n",
    "if \"google/gemma\" in MODELO.lower():\n",
    "    print(\"Configurando vLLM para modelo Gemma...\")\n",
    "    try:\n",
    "        llm = LLM(\n",
    "            model=modelo_local, \n",
    "            gpu_memory_utilization=0.7, \n",
    "            dtype=\"float16\", \n",
    "            trust_remote_code=True,\n",
    "            # Configuraciones adicionales para Gemma\n",
    "            max_model_len=4096,\n",
    "            enforce_eager=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error con configuración específica de Gemma: {e}\")\n",
    "        print(\"Intentando con configuración estándar...\")\n",
    "        llm = LLM(model=modelo_local, gpu_memory_utilization=0.7, dtype=\"float16\", trust_remote_code=True)\n",
    "else:\n",
    "    llm = LLM(model=modelo_local, gpu_memory_utilization=0.7, dtype=\"float16\", trust_remote_code=True)\n",
    "\n",
    "print(\"¡Modelo cargado! Iniciando generación de respuestas...\")\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "print(\"¡Generación completa! Evaluando respuestas...\")\n",
    "\n",
    "total_preguntas = 0\n",
    "correctas = 0\n",
    "resultados_evaluacion = []\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    pregunta_original = benchmark_sample[i]['pregunta']\n",
    "    respuesta_llm = output.outputs[0].text.strip()\n",
    "    respuesta_correcta_gt = respuestas_correctas_sample[i]\n",
    "\n",
    "    # Verificar que la respuesta del LLM esté en español\n",
    "    respuesta_en_espanol = respuesta_llm\n",
    "    \n",
    "    norm_correcta = normalizar_texto(respuesta_correcta_gt)\n",
    "    norm_llm = normalizar_texto(respuesta_llm)\n",
    "    es_correcta = (norm_correcta in norm_llm) and (norm_correcta != \"\") and respuesta_en_espanol\n",
    "\n",
    "    if es_correcta:\n",
    "        correctas += 1\n",
    "    total_preguntas += 1\n",
    "\n",
    "    evaluacion = \"CORRECTO\" if es_correcta else \"INCORRECTO\"\n",
    "    if not respuesta_en_espanol:\n",
    "        evaluacion += \" (No español)\"\n",
    "\n",
    "    resultados_evaluacion.append({\n",
    "        \"pregunta\": pregunta_original,\n",
    "        \"respuesta_correcta\": respuesta_correcta_gt,\n",
    "        \"respuesta_llm\": respuesta_llm,\n",
    "        \"evaluacion\": evaluacion,\n",
    "        \"respuesta_en_espanol\": respuesta_en_espanol\n",
    "    })\n",
    "\n",
    "print(\"\\n--- Tabla de Resultados de Evaluación (Muestra de 10) ---\")\n",
    "for res in resultados_evaluacion[:10]:\n",
    "    print(f\"  P: {res['pregunta']}\")\n",
    "    print(f\"  R. Correcta: {res['respuesta_correcta']}\")\n",
    "    print(f\"  R. LLM: {res['respuesta_llm']} -> {res['evaluacion']}\")\n",
    "    print(\"  ---\")\n",
    "\n",
    "if total_preguntas > 0:\n",
    "    puntaje = (correctas / total_preguntas) * 100\n",
    "    respuestas_espanol = sum(1 for r in resultados_evaluacion if r['respuesta_en_espanol'])\n",
    "    print(f\"\\n Resultado Final ({MODELO}): {correctas} de {total_preguntas} correctas ({puntaje:.1f}%)\")\n",
    "    print(f\" Respuestas en español: {respuestas_espanol} de {total_preguntas} ({(respuestas_espanol/total_preguntas)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"No se evaluaron preguntas.\")\n",
    "\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_evaluacion, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Resultados detallados guardados en '{OUTPUT_FILE}'\")\n",
    "print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from unidecode import unidecode\n",
    "from vllm import LLM, SamplingParams\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer\n",
    "from langdetect import detect, detect_langs\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "# ------------ CONFIG ------------\n",
    "MODELO = \"Qwen/Qwen3-4B-Instruct-2507\"   # repo HF o ruta local meta-llama/Llama-3.1-8B-Instruct TinyLlama/TinyLlama-1.1B-Chat-v1.0 Qwen/Qwen3-4B-Instruct-2507\n",
    "CANTIDAD = 100000\n",
    "MAX_PREGUNTAS_FILTRAR = 100000  # Limitar el número de preguntas a procesar para filtrar\n",
    "BENCHMARK_FILE = \"tomy_qa.json\"\n",
    "OUTPUT_FILE = f\"tomy_evaluacion_real_{MODELO.split('/')[-1]}.json\"\n",
    "MODELS_DIR = Path(\"./models\")                   # dónde guardar los modelos\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"\"\n",
    "# ---------------------------------\n",
    "\n",
    "print(f\"Configuración:\")\n",
    "print(f\"- Modelo: {MODELO}\")\n",
    "print(f\"- Cantidad de preguntas para evaluación: {CANTIDAD}\")\n",
    "print(f\"- Máximo de preguntas a procesar para filtrar: {MAX_PREGUNTAS_FILTRAR}\")\n",
    "print(f\"- Archivo benchmark: {BENCHMARK_FILE}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Ejemplos few-shot específicos de Latinoamérica\n",
    "FEW_SHOT_EXAMPLES = [\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Argentina?\",\n",
    "        \"respuesta\": \"Buenos Aires\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué año se independizó México?\",\n",
    "        \"respuesta\": \"1810\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la moneda oficial de Chile?\",\n",
    "        \"respuesta\": \"Peso chileno\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Quién fue el libertador de Venezuela?\",\n",
    "        \"respuesta\": \"Simón Bolívar\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es el río más largo de Sudamérica?\",\n",
    "        \"respuesta\": \"Río Amazonas\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué país se encuentra Machu Picchu?\",\n",
    "        \"respuesta\": \"Perú\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Colombia?\",\n",
    "        \"respuesta\": \"Bogotá\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Qué escritor colombiano ganó el Premio Nobel de Literatura?\",\n",
    "        \"respuesta\": \"Gabriel García Márquez\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es el país más grande de América del Sur?\",\n",
    "        \"respuesta\": \"Brasil\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué océano se encuentran las Islas Galápagos?\",\n",
    "        \"respuesta\": \"Océano Pacífico\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Ecuador?\",\n",
    "        \"respuesta\": \"Quito\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Qué cordillera atraviesa América del Sur?\",\n",
    "        \"respuesta\": \"Cordillera de los Andes\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la moneda oficial de Brasil?\",\n",
    "        \"respuesta\": \"Real brasileño\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué país nació el tango?\",\n",
    "        \"respuesta\": \"Argentina\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Uruguay?\",\n",
    "        \"respuesta\": \"Montevideo\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Qué país tiene forma de bota en América del Sur?\",\n",
    "        \"respuesta\": \"Chile\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es el desierto más árido del mundo ubicado en Chile?\",\n",
    "        \"respuesta\": \"Desierto de Atacama\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué país se encuentra el Salar de Uyuni?\",\n",
    "        \"respuesta\": \"Bolivia\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Paraguay?\",\n",
    "        \"respuesta\": \"Asunción\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Qué idioma se habla en Brasil?\",\n",
    "        \"respuesta\": \"Portugués\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def is_local_path(p: str) -> bool:\n",
    "    return Path(p).expanduser().exists()\n",
    "\n",
    "def sanitize(name: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in \"._-+=\" else \"_\" for ch in name)\n",
    "\n",
    "def ensure_local_model(model_id_or_path: str, local_dir: Path) -> str:\n",
    "    if is_local_path(model_id_or_path):\n",
    "        return str(Path(model_id_or_path).expanduser().resolve())\n",
    "    \n",
    "    local_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dst = local_dir / sanitize(model_id_or_path)\n",
    "    \n",
    "    # Bypass específico para google/gemma-3-4b-it\n",
    "    if \"google/gemma\" in model_id_or_path.lower():\n",
    "        print(f\"Aplicando bypass para modelo Gemma: {model_id_or_path}\")\n",
    "        try:\n",
    "            # Intentar descarga con patrones más amplios para Gemma\n",
    "            snapshot_download(\n",
    "                repo_id=model_id_or_path,\n",
    "                local_dir=str(dst),\n",
    "                local_dir_use_symlinks=False,\n",
    "                # Patrones más amplios para Gemma\n",
    "                allow_patterns=[\n",
    "                    \"*.json\", \"*.txt\", \"*.model\", \"*.safetensors\", \"*.bin\",\n",
    "                    \"tokenizer*\", \"config*\", \"generation_config*\",\n",
    "                    \"model*\", \"pytorch_model*\", \"special_tokens_map*\"\n",
    "                ],\n",
    "            )\n",
    "        except Exception as e1:\n",
    "            print(f\"Primer intento falló: {e1}\")\n",
    "            try:\n",
    "                # Segundo intento: descarga completa sin filtros\n",
    "                print(\"Intentando descarga completa sin filtros...\")\n",
    "                snapshot_download(\n",
    "                    repo_id=model_id_or_path,\n",
    "                    local_dir=str(dst),\n",
    "                    local_dir_use_symlinks=False,\n",
    "                )\n",
    "            except Exception as e2:\n",
    "                print(f\"Segundo intento falló: {e2}\")\n",
    "                # Tercer intento: usar directamente el repo_id para vLLM\n",
    "                print(\"Usando repo_id directamente para vLLM...\")\n",
    "                return model_id_or_path\n",
    "    else:\n",
    "        # Para otros modelos, usar el método original\n",
    "        snapshot_download(\n",
    "            repo_id=model_id_or_path,\n",
    "            local_dir=str(dst),\n",
    "            local_dir_use_symlinks=False,\n",
    "            allow_patterns=[\n",
    "                \"config.json\", \"generation_config.json\",\n",
    "                \"tokenizer.json\", \"tokenizer.model\", \"tokenizer_config.json\",\n",
    "                \"special_tokens_map.json\", \"merges.txt\", \"vocab.json\",\n",
    "                \"*.safetensors\", \"model.safetensors.index.json\", \"pytorch_model.bin\",\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "    return str(dst)\n",
    "\n",
    "def is_spanish_text(text: str) -> bool:\n",
    "    \"\"\"Detecta si el texto está en español\"\"\"\n",
    "    if not text or len(text.strip()) < 3:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Usar detect_langs para obtener probabilidades\n",
    "        languages = detect_langs(text)\n",
    "        \n",
    "        # Buscar español en los resultados\n",
    "        for lang in languages:\n",
    "            if lang.lang == 'es' and lang.prob > 0.55:\n",
    "                return True\n",
    "        \n",
    "        # Si no hay alta confianza, usar detect simple\n",
    "        detected_lang = detect(text)\n",
    "        return detected_lang == 'es'\n",
    "        \n",
    "    except:\n",
    "        # Si langdetect falla completamente, considerar como no español\n",
    "        return False\n",
    "\n",
    "def filter_spanish_questions_sequential(benchmark_data, max_items=None):\n",
    "    \"\"\"Filtra preguntas que estén completamente en español usando procesamiento secuencial\"\"\"\n",
    "    filtered_data = []\n",
    "    filtered_count = 0\n",
    "    \n",
    "    # Limitar el número de items a procesar si se especifica\n",
    "    if max_items and max_items < len(benchmark_data):\n",
    "        print(f\"Limitando procesamiento a {max_items} de {len(benchmark_data)} preguntas totales\")\n",
    "        data_to_process = benchmark_data[:max_items]\n",
    "    else:\n",
    "        data_to_process = benchmark_data\n",
    "    \n",
    "    print(f\"Filtrando {len(data_to_process)} preguntas...\")\n",
    "    \n",
    "    for item in tqdm(data_to_process, desc=\"Filtrando preguntas\"):\n",
    "        pregunta = item.get('pregunta', '')\n",
    "        respuesta = item.get('respuesta_correcta', '')\n",
    "        \n",
    "        # Verificar que tanto pregunta como respuesta estén en español\n",
    "        if is_spanish_text(pregunta) and is_spanish_text(respuesta):\n",
    "            filtered_data.append(item)\n",
    "        else:\n",
    "            filtered_count += 1\n",
    "    \n",
    "    print(f\"Filtradas {filtered_count} preguntas (no español)\")\n",
    "    print(f\"Preguntas válidas en español: {len(filtered_data)}\")\n",
    "    return filtered_data\n",
    "\n",
    "def normalizar_texto(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return \"\"\n",
    "    texto = texto.lower()\n",
    "    texto = unidecode(texto)\n",
    "    texto = texto.translate(str.maketrans('', '', string.punctuation))\n",
    "    return texto.strip()\n",
    "\n",
    "# --- Carga benchmark ---\n",
    "if not os.path.exists(BENCHMARK_FILE):\n",
    "    print(f\"Error: No se encuentra '{BENCHMARK_FILE}'. Ejecuta el script 2 primero.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "with open(BENCHMARK_FILE, 'r', encoding='utf-8') as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "if len(benchmark_data) == 0:\n",
    "    print(\"Error: El benchmark está vacío.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Filtrar preguntas en español usando procesamiento secuencial\n",
    "print(f\"Datos originales: {len(benchmark_data)} preguntas\")\n",
    "# benchmark_data_spanish = filter_spanish_questions_sequential(benchmark_data, MAX_PREGUNTAS_FILTRAR)\n",
    "benchmark_data_spanish = benchmark_data\n",
    "print(f\"Después del filtro de español: {len(benchmark_data_spanish)} preguntas\")\n",
    "\n",
    "if len(benchmark_data_spanish) == 0:\n",
    "    print(\"Error: No hay preguntas válidas en español después del filtro.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Verificar si tenemos suficientes preguntas para la muestra\n",
    "if len(benchmark_data_spanish) < CANTIDAD:\n",
    "    print(f\"Advertencia: Solo hay {len(benchmark_data_spanish)} preguntas válidas, usando todas para la evaluación.\")\n",
    "    CANTIDAD = len(benchmark_data_spanish)\n",
    "\n",
    "benchmark_sample = random.sample(benchmark_data_spanish, min(len(benchmark_data_spanish), CANTIDAD))\n",
    "print(f\"Muestra seleccionada: {len(benchmark_sample)} preguntas\")\n",
    "\n",
    "# --- Modelo local (descarga si hace falta) ---\n",
    "modelo_local = ensure_local_model(MODELO, MODELS_DIR)\n",
    "print(f\"Modelo preparado en: {modelo_local}\")\n",
    "\n",
    "# --- Tokenizer (para chat template si existe) ---\n",
    "# Bypass para tokenizer de Gemma\n",
    "if \"google/gemma\" in MODELO.lower() and modelo_local == MODELO:\n",
    "    # Si no se pudo descargar localmente, usar directamente el repo\n",
    "    print(\"Cargando tokenizer directamente desde Hugging Face...\")\n",
    "    tok = AutoTokenizer.from_pretrained(\n",
    "        MODELO,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    tok = AutoTokenizer.from_pretrained(\n",
    "        modelo_local,\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True\n",
    "    )\n",
    "\n",
    "def create_few_shot_prompt(pregunta: str) -> str:\n",
    "    \"\"\"Crea un prompt con ejemplos few-shot de Latinoamérica\"\"\"\n",
    "    system_message = \"\"\"Eres un asistente experto en conocimientos de Latinoamérica. Responde de manera precisa y concisa en español.\n",
    "\n",
    "Ejemplos de preguntas y respuestas sobre Latinoamérica:\"\"\"\n",
    "    \n",
    "    # Agregar ejemplos few-shot\n",
    "    examples_text = \"\"\n",
    "    for example in FEW_SHOT_EXAMPLES:\n",
    "        examples_text += f\"\\nPregunta: {example['pregunta']}\\nRespuesta: {example['respuesta']}\\n\"\n",
    "    \n",
    "    user_message = f\"\\nAhora responde esta pregunta:\\nPregunta: {pregunta}\\nRespuesta:\"\n",
    "    \n",
    "    return system_message + examples_text + user_message\n",
    "\n",
    "def format_prompt(pregunta: str) -> str:\n",
    "    few_shot_content = create_few_shot_prompt(pregunta)\n",
    "    \n",
    "    if hasattr(tok, \"apply_chat_template\") and (getattr(tok, \"chat_template\", None) is not None):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Eres un asistente experto en conocimientos de Latinoamérica.\"},\n",
    "            {\"role\": \"user\", \"content\": few_shot_content}\n",
    "        ]\n",
    "        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return f\"<|user|>\\n{few_shot_content}<|assistant|>\"\n",
    "\n",
    "prompts = [format_prompt(item['pregunta']) for item in benchmark_sample]\n",
    "respuestas_correctas_sample = [item['respuesta_correcta'] for item in benchmark_sample]\n",
    "\n",
    "print(f\"\\nCargando modelo '{MODELO}' en la GPU...\")\n",
    "sampling_params = SamplingParams(temperature=0.1, top_p=0.9, max_tokens=128)\n",
    "\n",
    "# Configuración específica para Gemma\n",
    "if \"google/gemma\" in MODELO.lower():\n",
    "    print(\"Configurando vLLM para modelo Gemma...\")\n",
    "    try:\n",
    "        llm = LLM(\n",
    "            model=modelo_local, \n",
    "            gpu_memory_utilization=0.7, \n",
    "            dtype=\"float16\", \n",
    "            trust_remote_code=True,\n",
    "            # Configuraciones adicionales para Gemma\n",
    "            max_model_len=4096,\n",
    "            enforce_eager=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error con configuración específica de Gemma: {e}\")\n",
    "        print(\"Intentando con configuración estándar...\")\n",
    "        llm = LLM(model=modelo_local, gpu_memory_utilization=0.7, dtype=\"float16\", trust_remote_code=True)\n",
    "else:\n",
    "    llm = LLM(model=modelo_local, gpu_memory_utilization=0.7, dtype=\"float16\", trust_remote_code=True)\n",
    "\n",
    "print(\"¡Modelo cargado! Iniciando generación de respuestas...\")\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "print(\"¡Generación completa! Evaluando respuestas...\")\n",
    "\n",
    "total_preguntas = 0\n",
    "correctas = 0\n",
    "resultados_evaluacion = []\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    pregunta_original = benchmark_sample[i]['pregunta']\n",
    "    respuesta_llm = output.outputs[0].text.strip()\n",
    "    respuesta_correcta_gt = respuestas_correctas_sample[i]\n",
    "\n",
    "    # Verificar que la respuesta del LLM esté en español\n",
    "    respuesta_en_espanol = respuesta_llm\n",
    "    \n",
    "    norm_correcta = normalizar_texto(respuesta_correcta_gt)\n",
    "    norm_llm = normalizar_texto(respuesta_llm)\n",
    "    es_correcta = (norm_correcta in norm_llm) and (norm_correcta != \"\") and respuesta_en_espanol\n",
    "\n",
    "    if es_correcta:\n",
    "        correctas += 1\n",
    "    total_preguntas += 1\n",
    "\n",
    "    evaluacion = \"CORRECTO\" if es_correcta else \"INCORRECTO\"\n",
    "    if not respuesta_en_espanol:\n",
    "        evaluacion += \" (No español)\"\n",
    "\n",
    "    resultados_evaluacion.append({\n",
    "        \"pregunta\": pregunta_original,\n",
    "        \"respuesta_correcta\": respuesta_correcta_gt,\n",
    "        \"respuesta_llm\": respuesta_llm,\n",
    "        \"evaluacion\": evaluacion,\n",
    "        \"respuesta_en_espanol\": respuesta_en_espanol\n",
    "    })\n",
    "\n",
    "print(\"\\n--- Tabla de Resultados de Evaluación (Muestra de 10) ---\")\n",
    "for res in resultados_evaluacion[:10]:\n",
    "    print(f\"  P: {res['pregunta']}\")\n",
    "    print(f\"  R. Correcta: {res['respuesta_correcta']}\")\n",
    "    print(f\"  R. LLM: {res['respuesta_llm']} -> {res['evaluacion']}\")\n",
    "    print(\"  ---\")\n",
    "\n",
    "if total_preguntas > 0:\n",
    "    puntaje = (correctas / total_preguntas) * 100\n",
    "    respuestas_espanol = sum(1 for r in resultados_evaluacion if r['respuesta_en_espanol'])\n",
    "    print(f\"\\n Resultado Final ({MODELO}): {correctas} de {total_preguntas} correctas ({puntaje:.1f}%)\")\n",
    "    print(f\" Respuestas en español: {respuestas_espanol} de {total_preguntas} ({(respuestas_espanol/total_preguntas)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"No se evaluaron preguntas.\")\n",
    "\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_evaluacion, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Resultados detallados guardados en '{OUTPUT_FILE}'\")\n",
    "print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from unidecode import unidecode\n",
    "from vllm import LLM, SamplingParams\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer\n",
    "from langdetect import detect, detect_langs\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# ------------ CONFIG ------------\n",
    "MODELO = \"Qwen/Qwen3-4B-Instruct-2507\"   # repo HF o ruta local meta-llama/Llama-3.1-8B-Instruct TinyLlama/TinyLlama-1.1B-Chat-v1.0 Qwen/Qwen3-4B-Instruct-2507\n",
    "CANTIDAD = 500000\n",
    "MAX_PREGUNTAS_FILTRAR = 500000  # Limitar el número de preguntas a procesar para filtrar\n",
    "BENCHMARK_FILE = \"justo_qa.json\"\n",
    "OUTPUT_FILE = f\"justo_evaluacion_real_{MODELO.split('/')[-1]}.json\"\n",
    "MODELS_DIR = Path(\"./models\")                   # dónde guardar los modelos\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"\"\n",
    "# ---------------------------------\n",
    "\n",
    "print(f\"Configuración:\")\n",
    "print(f\"- Modelo: {MODELO}\")\n",
    "print(f\"- Cantidad de preguntas para evaluación: {CANTIDAD}\")\n",
    "print(f\"- Máximo de preguntas a procesar para filtrar: {MAX_PREGUNTAS_FILTRAR}\")\n",
    "print(f\"- Archivo benchmark: {BENCHMARK_FILE}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Ejemplos few-shot específicos de Latinoamérica\n",
    "FEW_SHOT_EXAMPLES = [\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Argentina?\",\n",
    "        \"respuesta\": \"Buenos Aires\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué año se independizó México?\",\n",
    "        \"respuesta\": \"1810\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la moneda oficial de Chile?\",\n",
    "        \"respuesta\": \"Peso chileno\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Quién fue el libertador de Venezuela?\",\n",
    "        \"respuesta\": \"Simón Bolívar\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es el río más largo de Sudamérica?\",\n",
    "        \"respuesta\": \"Río Amazonas\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué país se encuentra Machu Picchu?\",\n",
    "        \"respuesta\": \"Perú\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Colombia?\",\n",
    "        \"respuesta\": \"Bogotá\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Qué escritor colombiano ganó el Premio Nobel de Literatura?\",\n",
    "        \"respuesta\": \"Gabriel García Márquez\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es el país más grande de América del Sur?\",\n",
    "        \"respuesta\": \"Brasil\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué océano se encuentran las Islas Galápagos?\",\n",
    "        \"respuesta\": \"Océano Pacífico\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Ecuador?\",\n",
    "        \"respuesta\": \"Quito\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Qué cordillera atraviesa América del Sur?\",\n",
    "        \"respuesta\": \"Cordillera de los Andes\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la moneda oficial de Brasil?\",\n",
    "        \"respuesta\": \"Real brasileño\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué país nació el tango?\",\n",
    "        \"respuesta\": \"Argentina\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Uruguay?\",\n",
    "        \"respuesta\": \"Montevideo\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Qué país tiene forma de bota en América del Sur?\",\n",
    "        \"respuesta\": \"Chile\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es el desierto más árido del mundo ubicado en Chile?\",\n",
    "        \"respuesta\": \"Desierto de Atacama\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿En qué país se encuentra el Salar de Uyuni?\",\n",
    "        \"respuesta\": \"Bolivia\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Cuál es la capital de Paraguay?\",\n",
    "        \"respuesta\": \"Asunción\"\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"¿Qué idioma se habla en Brasil?\",\n",
    "        \"respuesta\": \"Portugués\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def is_local_path(p: str) -> bool:\n",
    "    return Path(p).expanduser().exists()\n",
    "\n",
    "def sanitize(name: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in \"._-+=\" else \"_\" for ch in name)\n",
    "\n",
    "def ensure_local_model(model_id_or_path: str, local_dir: Path) -> str:\n",
    "    if is_local_path(model_id_or_path):\n",
    "        return str(Path(model_id_or_path).expanduser().resolve())\n",
    "    \n",
    "    local_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dst = local_dir / sanitize(model_id_or_path)\n",
    "    \n",
    "    # Bypass específico para google/gemma-3-4b-it\n",
    "    if \"google/gemma\" in model_id_or_path.lower():\n",
    "        print(f\"Aplicando bypass para modelo Gemma: {model_id_or_path}\")\n",
    "        try:\n",
    "            # Intentar descarga con patrones más amplios para Gemma\n",
    "            snapshot_download(\n",
    "                repo_id=model_id_or_path,\n",
    "                local_dir=str(dst),\n",
    "                local_dir_use_symlinks=False,\n",
    "                # Patrones más amplios para Gemma\n",
    "                allow_patterns=[\n",
    "                    \"*.json\", \"*.txt\", \"*.model\", \"*.safetensors\", \"*.bin\",\n",
    "                    \"tokenizer*\", \"config*\", \"generation_config*\",\n",
    "                    \"model*\", \"pytorch_model*\", \"special_tokens_map*\"\n",
    "                ],\n",
    "            )\n",
    "        except Exception as e1:\n",
    "            print(f\"Primer intento falló: {e1}\")\n",
    "            try:\n",
    "                # Segundo intento: descarga completa sin filtros\n",
    "                print(\"Intentando descarga completa sin filtros...\")\n",
    "                snapshot_download(\n",
    "                    repo_id=model_id_or_path,\n",
    "                    local_dir=str(dst),\n",
    "                    local_dir_use_symlinks=False,\n",
    "                )\n",
    "            except Exception as e2:\n",
    "                print(f\"Segundo intento falló: {e2}\")\n",
    "                # Tercer intento: usar directamente el repo_id para vLLM\n",
    "                print(\"Usando repo_id directamente para vLLM...\")\n",
    "                return model_id_or_path\n",
    "    else:\n",
    "        # Para otros modelos, usar el método original\n",
    "        snapshot_download(\n",
    "            repo_id=model_id_or_path,\n",
    "            local_dir=str(dst),\n",
    "            local_dir_use_symlinks=False,\n",
    "            allow_patterns=[\n",
    "                \"config.json\", \"generation_config.json\",\n",
    "                \"tokenizer.json\", \"tokenizer.model\", \"tokenizer_config.json\",\n",
    "                \"special_tokens_map.json\", \"merges.txt\", \"vocab.json\",\n",
    "                \"*.safetensors\", \"model.safetensors.index.json\", \"pytorch_model.bin\",\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "    return str(dst)\n",
    "\n",
    "def is_spanish_text(text: str) -> bool:\n",
    "    \"\"\"Detecta si el texto está en español\"\"\"\n",
    "    if not text or len(text.strip()) < 3:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Usar detect_langs para obtener probabilidades\n",
    "        languages = detect_langs(text)\n",
    "        \n",
    "        # Buscar español en los resultados\n",
    "        for lang in languages:\n",
    "            if lang.lang == 'es' and lang.prob > 0.55:\n",
    "                return True\n",
    "        \n",
    "        # Si no hay alta confianza, usar detect simple\n",
    "        detected_lang = detect(text)\n",
    "        return detected_lang == 'es'\n",
    "        \n",
    "    except:\n",
    "        # Si langdetect falla completamente, considerar como no español\n",
    "        return False\n",
    "\n",
    "def filter_spanish_questions_sequential(benchmark_data, max_items=None):\n",
    "    \"\"\"Filtra preguntas que estén completamente en español usando procesamiento secuencial\"\"\"\n",
    "    filtered_data = []\n",
    "    filtered_count = 0\n",
    "    \n",
    "    # Limitar el número de items a procesar si se especifica\n",
    "    if max_items and max_items < len(benchmark_data):\n",
    "        print(f\"Limitando procesamiento a {max_items} de {len(benchmark_data)} preguntas totales\")\n",
    "        data_to_process = benchmark_data[:max_items]\n",
    "    else:\n",
    "        data_to_process = benchmark_data\n",
    "    \n",
    "    print(f\"Filtrando {len(data_to_process)} preguntas...\")\n",
    "    \n",
    "    for item in tqdm(data_to_process, desc=\"Filtrando preguntas\"):\n",
    "        pregunta = item.get('pregunta', '')\n",
    "        respuesta = item.get('respuesta_correcta', '')\n",
    "        \n",
    "        # Verificar que tanto pregunta como respuesta estén en español\n",
    "        if is_spanish_text(pregunta) and is_spanish_text(respuesta):\n",
    "            filtered_data.append(item)\n",
    "        else:\n",
    "            filtered_count += 1\n",
    "    \n",
    "    print(f\"Filtradas {filtered_count} preguntas (no español)\")\n",
    "    print(f\"Preguntas válidas en español: {len(filtered_data)}\")\n",
    "    return filtered_data\n",
    "\n",
    "def normalizar_texto(texto):\n",
    "    if not isinstance(texto, str):\n",
    "        return \"\"\n",
    "    texto = texto.lower()\n",
    "    texto = unidecode(texto)\n",
    "    texto = texto.translate(str.maketrans('', '', string.punctuation))\n",
    "    return texto.strip()\n",
    "\n",
    "# --- Carga benchmark ---\n",
    "if not os.path.exists(BENCHMARK_FILE):\n",
    "    print(f\"Error: No se encuentra '{BENCHMARK_FILE}'. Ejecuta el script 2 primero.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "with open(BENCHMARK_FILE, 'r', encoding='utf-8') as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "if len(benchmark_data) == 0:\n",
    "    print(\"Error: El benchmark está vacío.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Filtrar preguntas en español usando procesamiento secuencial\n",
    "print(f\"Datos originales: {len(benchmark_data)} preguntas\")\n",
    "# benchmark_data_spanish = filter_spanish_questions_sequential(benchmark_data, MAX_PREGUNTAS_FILTRAR)\n",
    "benchmark_data_spanish = benchmark_data\n",
    "print(f\"Después del filtro de español: {len(benchmark_data_spanish)} preguntas\")\n",
    "\n",
    "if len(benchmark_data_spanish) == 0:\n",
    "    print(\"Error: No hay preguntas válidas en español después del filtro.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Verificar si tenemos suficientes preguntas para la muestra\n",
    "if len(benchmark_data_spanish) < CANTIDAD:\n",
    "    print(f\"Advertencia: Solo hay {len(benchmark_data_spanish)} preguntas válidas, usando todas para la evaluación.\")\n",
    "    CANTIDAD = len(benchmark_data_spanish)\n",
    "\n",
    "benchmark_sample = random.sample(benchmark_data_spanish, min(len(benchmark_data_spanish), CANTIDAD))\n",
    "print(f\"Muestra seleccionada: {len(benchmark_sample)} preguntas\")\n",
    "\n",
    "# --- Modelo local (descarga si hace falta) ---\n",
    "modelo_local = ensure_local_model(MODELO, MODELS_DIR)\n",
    "print(f\"Modelo preparado en: {modelo_local}\")\n",
    "\n",
    "# --- Tokenizer (para chat template si existe) ---\n",
    "# Bypass para tokenizer de Gemma\n",
    "if \"google/gemma\" in MODELO.lower() and modelo_local == MODELO:\n",
    "    # Si no se pudo descargar localmente, usar directamente el repo\n",
    "    print(\"Cargando tokenizer directamente desde Hugging Face...\")\n",
    "    tok = AutoTokenizer.from_pretrained(\n",
    "        MODELO,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    tok = AutoTokenizer.from_pretrained(\n",
    "        modelo_local,\n",
    "        trust_remote_code=True,\n",
    "        local_files_only=True\n",
    "    )\n",
    "\n",
    "def create_few_shot_prompt(pregunta: str) -> str:\n",
    "    \"\"\"Crea un prompt con ejemplos few-shot de Latinoamérica\"\"\"\n",
    "    system_message = \"\"\"Eres un asistente experto en conocimientos de Latinoamérica. Responde de manera precisa y concisa en español.\n",
    "\n",
    "Ejemplos de preguntas y respuestas sobre Latinoamérica:\"\"\"\n",
    "    \n",
    "    # Agregar ejemplos few-shot\n",
    "    examples_text = \"\"\n",
    "    for example in FEW_SHOT_EXAMPLES:\n",
    "        examples_text += f\"\\nPregunta: {example['pregunta']}\\nRespuesta: {example['respuesta']}\\n\"\n",
    "    \n",
    "    user_message = f\"\\nAhora responde esta pregunta:\\nPregunta: {pregunta}\\nRespuesta:\"\n",
    "    \n",
    "    return system_message + examples_text + user_message\n",
    "\n",
    "def format_prompt(pregunta: str) -> str:\n",
    "    few_shot_content = create_few_shot_prompt(pregunta)\n",
    "    \n",
    "    if hasattr(tok, \"apply_chat_template\") and (getattr(tok, \"chat_template\", None) is not None):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Eres un asistente experto en conocimientos de Latinoamérica.\"},\n",
    "            {\"role\": \"user\", \"content\": few_shot_content}\n",
    "        ]\n",
    "        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return f\"<|user|>\\n{few_shot_content}<|assistant|>\"\n",
    "\n",
    "prompts = [format_prompt(item['pregunta']) for item in benchmark_sample]\n",
    "respuestas_correctas_sample = [item['respuesta_correcta'] for item in benchmark_sample]\n",
    "\n",
    "print(f\"\\nCargando modelo '{MODELO}' en la GPU...\")\n",
    "sampling_params = SamplingParams(temperature=0.1, top_p=0.9, max_tokens=128)\n",
    "\n",
    "# Configuración específica para Gemma\n",
    "if \"google/gemma\" in MODELO.lower():\n",
    "    print(\"Configurando vLLM para modelo Gemma...\")\n",
    "    try:\n",
    "        llm = LLM(\n",
    "            model=modelo_local, \n",
    "            gpu_memory_utilization=0.9, \n",
    "            dtype=\"float16\", \n",
    "            trust_remote_code=True,\n",
    "            # Configuraciones adicionales para Gemma\n",
    "            max_model_len=4096,\n",
    "            enforce_eager=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error con configuración específica de Gemma: {e}\")\n",
    "        print(\"Intentando con configuración estándar...\")\n",
    "        llm = LLM(model=modelo_local, gpu_memory_utilization=0.9, dtype=\"float16\", trust_remote_code=True)\n",
    "else:\n",
    "    llm = LLM(model=modelo_local, gpu_memory_utilization=0.9, dtype=\"float16\", trust_remote_code=True, tensor_parallel_size=4)\n",
    "\n",
    "print(\"¡Modelo cargado! Iniciando generación de respuestas...\")\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "print(\"¡Generación completa! Evaluando respuestas...\")\n",
    "\n",
    "total_preguntas = 0\n",
    "correctas = 0\n",
    "resultados_evaluacion = []\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    pregunta_original = benchmark_sample[i]['pregunta']\n",
    "    respuesta_llm = output.outputs[0].text.strip()\n",
    "    respuesta_correcta_gt = respuestas_correctas_sample[i]\n",
    "\n",
    "    # Verificar que la respuesta del LLM esté en español\n",
    "    respuesta_en_espanol = respuesta_llm\n",
    "    \n",
    "    norm_correcta = normalizar_texto(respuesta_correcta_gt)\n",
    "    norm_llm = normalizar_texto(respuesta_llm)\n",
    "    es_correcta = (norm_correcta in norm_llm) and (norm_correcta != \"\") and respuesta_en_espanol\n",
    "\n",
    "    if es_correcta:\n",
    "        correctas += 1\n",
    "    total_preguntas += 1\n",
    "\n",
    "    evaluacion = \"CORRECTO\" if es_correcta else \"INCORRECTO\"\n",
    "    if not respuesta_en_espanol:\n",
    "        evaluacion += \" (No español)\"\n",
    "\n",
    "    resultados_evaluacion.append({\n",
    "        \"pregunta\": pregunta_original,\n",
    "        \"respuesta_correcta\": respuesta_correcta_gt,\n",
    "        \"respuesta_llm\": respuesta_llm,\n",
    "        \"evaluacion\": evaluacion,\n",
    "        \"respuesta_en_espanol\": respuesta_en_espanol\n",
    "    })\n",
    "\n",
    "print(\"\\n--- Tabla de Resultados de Evaluación (Muestra de 10) ---\")\n",
    "for res in resultados_evaluacion[:10]:\n",
    "    print(f\"  P: {res['pregunta']}\")\n",
    "    print(f\"  R. Correcta: {res['respuesta_correcta']}\")\n",
    "    print(f\"  R. LLM: {res['respuesta_llm']} -> {res['evaluacion']}\")\n",
    "    print(\"  ---\")\n",
    "\n",
    "if total_preguntas > 0:\n",
    "    puntaje = (correctas / total_preguntas) * 100\n",
    "    respuestas_espanol = sum(1 for r in resultados_evaluacion if r['respuesta_en_espanol'])\n",
    "    print(f\"\\n Resultado Final ({MODELO}): {correctas} de {total_preguntas} correctas ({puntaje:.1f}%)\")\n",
    "    print(f\" Respuestas en español: {respuestas_espanol} de {total_preguntas} ({(respuestas_espanol/total_preguntas)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"No se evaluaron preguntas.\")\n",
    "\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_evaluacion, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Resultados detallados guardados en '{OUTPUT_FILE}'\")\n",
    "print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0073d3f268b14de89a0877a9262a1d4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05354b8961834cc98a8fff8990760026": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07764b66d9fa43ff8a6b30ad172e2c8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08848770cff14b099f8d4f931dcbf89a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ab01a80a6d94d06b8b92979197225c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0073d3f268b14de89a0877a9262a1d4f",
      "placeholder": "​",
      "style": "IPY_MODEL_983f649c4dfa4231a74b78135bcf9855",
      "value": " 187/187 [00:00&lt;00:00, 18.6kB/s]"
     }
    },
    "0edbe6d36fae449196797abb5b1259f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f26cc2644e1b43deaf25f4638467a470",
       "IPY_MODEL_58b12cc2d61346a293c78f1a463a9e13",
       "IPY_MODEL_1e66a9ec08f44d6a81f9cdae40d9bd33"
      ],
      "layout": "IPY_MODEL_08848770cff14b099f8d4f931dcbf89a"
     }
    },
    "11bd744ff749439eb1fbdc84c28002e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "198d18312ffb4850b7be5e8dfe8d35cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e66a9ec08f44d6a81f9cdae40d9bd33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7b8bdc76621452d9268106016661f0c",
      "placeholder": "​",
      "style": "IPY_MODEL_add58c832ab8418881c5e4e934b6ecf9",
      "value": " 838/838 [00:00&lt;00:00, 40.3kB/s]"
     }
    },
    "1fbd3f69cc9b47e08d8ded92f7521f23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb13eeccd2c648e4ad4e440b93d54279",
      "placeholder": "​",
      "style": "IPY_MODEL_aa87c6af1d6c455e905d1f01ff9e698f",
      "value": "generation_config.json: 100%"
     }
    },
    "220c1b5ceaa244c3aa1117073ef965c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "237b6ab63714426dbc8f409bddb35df6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e640266f3e404d9e8b13865eab497134",
       "IPY_MODEL_d36e34918bd247bb867a01010b33c995",
       "IPY_MODEL_2af70eea493c4f18853b996db5305cca"
      ],
      "layout": "IPY_MODEL_a90d88dc104b4115a892d5fa7ada7d8f"
     }
    },
    "24443d77db294988b74cb4cf7bbf26bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9551f6eac49140f4840e53fce5cd566f",
       "IPY_MODEL_2b8526cec8014f5b8f623e98e7789711",
       "IPY_MODEL_5eec4a10578b4add883a83a7b167f66f"
      ],
      "layout": "IPY_MODEL_9c1f901ca8284294a2472b2d8a519b72"
     }
    },
    "2af70eea493c4f18853b996db5305cca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c28ae5ff5cc442d484a38be7b0108b41",
      "placeholder": "​",
      "style": "IPY_MODEL_55dd37142d964bfd90d977b3c1f42f85",
      "value": " 17.5M/17.5M [00:00&lt;00:00, 17.8MB/s]"
     }
    },
    "2b8526cec8014f5b8f623e98e7789711": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb5f749a8d8a4c2b995dfcf04da1ef94",
      "max": 4241003,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f2c1ea6610394f8fb35d8cd417083440",
      "value": 4241003
     }
    },
    "2bd1a6e0d3a345b9a3c106060a1a0b1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2f987687ba224fd299108a607b1cd321": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fc1d9a6b5804b8ca549b0530247df54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4680311133c6479fa6f411cbb2eb77ae",
      "max": 46996,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_764568d22c524eedbca4a474f1272cad",
      "value": 46996
     }
    },
    "30a2aa94a89c4e0a9b9bbccae543ed9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_4ae6a689951947a4b9c2bf507f875680"
     }
    },
    "35224a16e81a4a5a9fee4f43d4661b8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "372897e5189a40d1a4d83bf486d26edf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ee8cd2934161439ca77878bf7e871faf",
       "IPY_MODEL_2fc1d9a6b5804b8ca549b0530247df54",
       "IPY_MODEL_c419a408e7e04142b185b3fb8615c3e6"
      ],
      "layout": "IPY_MODEL_e2fad0a83ee04dd28809c4331ba04505"
     }
    },
    "393cdde0ce0c447c9ba49c2258bad204": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57698dd625b049169ac47b038da39052",
      "placeholder": "​",
      "style": "IPY_MODEL_5c67a81d65f34ce8bfb4eba7fba9b530",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "3b6c8be018414b0f922c2d5fd167a21c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35224a16e81a4a5a9fee4f43d4661b8e",
      "placeholder": "​",
      "style": "IPY_MODEL_feb3b98f6845497ebd8bf77e0c226473",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "3b708becca2945edb9436903e97153af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3fc003525b6a4acfaccd3f7620bf0b80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4680311133c6479fa6f411cbb2eb77ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ae6a689951947a4b9c2bf507f875680": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "4b41e093a13c4f2fbdeb2fd73c59a370": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e27f662d23043b7b4e9262dbee9fe2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "53c0d08a2a1b463d9c21b7e71a093d5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "53ef2d806f8e4afbac4ad92b7a41113d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_c1d3a21f96684425b1da7eed32b3bd20",
      "style": "IPY_MODEL_c43a51807e914767abd927dd227eac00",
      "value": true
     }
    },
    "55dd37142d964bfd90d977b3c1f42f85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57698dd625b049169ac47b038da39052": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58b12cc2d61346a293c78f1a463a9e13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_220c1b5ceaa244c3aa1117073ef965c5",
      "max": 838,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4e27f662d23043b7b4e9262dbee9fe2c",
      "value": 838
     }
    },
    "59b468681ecd4769a0df1a183d6a9c29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1fbd3f69cc9b47e08d8ded92f7521f23",
       "IPY_MODEL_b658cba0e48742f587dba5674b5e35e2",
       "IPY_MODEL_0ab01a80a6d94d06b8b92979197225c8"
      ],
      "layout": "IPY_MODEL_198d18312ffb4850b7be5e8dfe8d35cd"
     }
    },
    "5c67a81d65f34ce8bfb4eba7fba9b530": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e496b4e5f0f48d98c0724d03fb19b10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5eec4a10578b4add883a83a7b167f66f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f987687ba224fd299108a607b1cd321",
      "placeholder": "​",
      "style": "IPY_MODEL_2bd1a6e0d3a345b9a3c106060a1a0b1d",
      "value": " 4.24M/4.24M [00:00&lt;00:00, 117kB/s]"
     }
    },
    "655fb02fb7fe4971a12c530e544a459d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e967a77a73e43aaac60203b58445d2d",
      "max": 636,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3b708becca2945edb9436903e97153af",
      "value": 636
     }
    },
    "6befc978134c4ec3b0e7fcc9e514e643": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad691c76caa54eb88b7515cda98bff2f",
      "placeholder": "​",
      "style": "IPY_MODEL_11bd744ff749439eb1fbdc84c28002e3",
      "value": " 636/636 [00:00&lt;00:00, 69.3kB/s]"
     }
    },
    "6e967a77a73e43aaac60203b58445d2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73c05d94f8db463ba83def081890ecec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "764568d22c524eedbca4a474f1272cad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "79da43237e974ef388a3d02967b1e48a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f82c5779ae942cea0dd848f9808474a",
      "placeholder": "​",
      "style": "IPY_MODEL_ad3e9817274a4d0ba1b604271139eca3",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "7d0989cb9f0f40409d7ca8ac6fe2bab8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f82c5779ae942cea0dd848f9808474a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "804122f957954c7da5f3e2ef30bbb8c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "804d394dcd6644e7bef11425cf0c6e53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "839672e39c6a4060928d9bf51e8bb200": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_4b41e093a13c4f2fbdeb2fd73c59a370",
      "placeholder": "​",
      "style": "IPY_MODEL_ea270910b1fe44f9bfba775d709b332d",
      "value": ""
     }
    },
    "860d799a96ac4d389dc1f230c47505b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "8e3d0217293348c2b03f76a5e38238d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9551f6eac49140f4840e53fce5cd566f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_804122f957954c7da5f3e2ef30bbb8c7",
      "placeholder": "​",
      "style": "IPY_MODEL_3fc003525b6a4acfaccd3f7620bf0b80",
      "value": "tokenizer.model: 100%"
     }
    },
    "983f649c4dfa4231a74b78135bcf9855": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c1f901ca8284294a2472b2d8a519b72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7b8bdc76621452d9268106016661f0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a90d88dc104b4115a892d5fa7ada7d8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa87c6af1d6c455e905d1f01ff9e698f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad3e9817274a4d0ba1b604271139eca3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad691c76caa54eb88b7515cda98bff2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "add58c832ab8418881c5e4e934b6ecf9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b658cba0e48742f587dba5674b5e35e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05354b8961834cc98a8fff8990760026",
      "max": 187,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_53c0d08a2a1b463d9c21b7e71a093d5c",
      "value": 187
     }
    },
    "b73e8fec140349089030cc00b7b1398d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3b6c8be018414b0f922c2d5fd167a21c",
       "IPY_MODEL_655fb02fb7fe4971a12c530e544a459d",
       "IPY_MODEL_6befc978134c4ec3b0e7fcc9e514e643"
      ],
      "layout": "IPY_MODEL_cfef150f2bdb4bac85914abd5d4da4a5"
     }
    },
    "c1d3a21f96684425b1da7eed32b3bd20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c28ae5ff5cc442d484a38be7b0108b41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c419a408e7e04142b185b3fb8615c3e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc5df19c2f794b368c4bab4f51f43a50",
      "placeholder": "​",
      "style": "IPY_MODEL_d2032b0236c94c6bbfe4884d8aeb8818",
      "value": " 47.0k/47.0k [00:00&lt;00:00, 3.64MB/s]"
     }
    },
    "c43a51807e914767abd927dd227eac00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c677c0c0f45642298d5eb499db872b50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c697cb26048147bca5c8b308da157574": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_5e496b4e5f0f48d98c0724d03fb19b10",
      "style": "IPY_MODEL_860d799a96ac4d389dc1f230c47505b9",
      "tooltip": ""
     }
    },
    "cb5f749a8d8a4c2b995dfcf04da1ef94": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc5df19c2f794b368c4bab4f51f43a50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfef150f2bdb4bac85914abd5d4da4a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2032b0236c94c6bbfe4884d8aeb8818": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d36e34918bd247bb867a01010b33c995": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7a011448a8142c689603fda3d4fd6ee",
      "max": 17525357,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_804d394dcd6644e7bef11425cf0c6e53",
      "value": 17525357
     }
    },
    "d88ba960b84949de8d700e7809624645": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d0989cb9f0f40409d7ca8ac6fe2bab8",
      "placeholder": "​",
      "style": "IPY_MODEL_de8b9177c99d4d198aecdbd9ab5d4d78",
      "value": "Connecting..."
     }
    },
    "de8b9177c99d4d198aecdbd9ab5d4d78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e2fad0a83ee04dd28809c4331ba04505": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e640266f3e404d9e8b13865eab497134": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c677c0c0f45642298d5eb499db872b50",
      "placeholder": "​",
      "style": "IPY_MODEL_73c05d94f8db463ba83def081890ecec",
      "value": "tokenizer.json: 100%"
     }
    },
    "ea270910b1fe44f9bfba775d709b332d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb13eeccd2c648e4ad4e440b93d54279": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee8cd2934161439ca77878bf7e871faf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07764b66d9fa43ff8a6b30ad172e2c8f",
      "placeholder": "​",
      "style": "IPY_MODEL_8e3d0217293348c2b03f76a5e38238d5",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "f1e728a31c8e4e2893637f6e27df200d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f26cc2644e1b43deaf25f4638467a470": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1e728a31c8e4e2893637f6e27df200d",
      "placeholder": "​",
      "style": "IPY_MODEL_f3590f6b727e451d9ab48a7c4f4ee428",
      "value": "config.json: 100%"
     }
    },
    "f2c1ea6610394f8fb35d8cd417083440": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f3590f6b727e451d9ab48a7c4f4ee428": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f7a011448a8142c689603fda3d4fd6ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "feb3b98f6845497ebd8bf77e0c226473": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
